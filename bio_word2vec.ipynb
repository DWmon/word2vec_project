{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Twitter, Kkma\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from copy import deepcopy\n",
    "import re\n",
    "import os\n",
    "from datetime import datetime\n",
    "import csv\n",
    "import sys \n",
    "\n",
    "t=Twitter()\n",
    "\n",
    "TXT_PATH='./bio_txt/'\n",
    "PDF_PATH='./bio_pdf/'\n",
    "SAVE_PATH='./bio_wv/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pdf_to_text(path):    #pdf 파일을 txt 파일로\n",
    "    pdf_list=[]\n",
    "    pdfs=os.listdir(PDF_PATH)\n",
    "    for pdf in pdfs:\n",
    "        ext = os.path.splitext(pdf)[-1]\n",
    "        name=os.path.splitext(pdf)[-2]+'.txt'\n",
    "        \n",
    "        if ext=='.pdf':\n",
    "            #pdf_list.append(\"%s%s\" % (path, pdf))\n",
    "            #pdf_name.append(\"%s\"%pdf)\n",
    "            os.system(\"python pdfminer.six-20170720/tools/pdf2txt.py -o %s%s %s%s\" %(TXT_PATH, name, PDF_PATH, pdf))\n",
    "\n",
    "def get_file(path): #txt파일 불러오기\n",
    "    filelist=[]\n",
    "    files = os.listdir(TXT_PATH) # txt파일에 있는 파일 리스트를 가져옴\n",
    "    for file in files:\n",
    "        ext = os.path.splitext(file)[-1]\n",
    "            \n",
    "        if ext == '.txt':\n",
    "            filelist.append(\"%s%s\" % (path, file))\n",
    "    \n",
    "    content=[]\n",
    "    for file in filelist:\n",
    "        content.append(preprocessing(open_file(file))) #content리스트 하나에 하나의 논문\n",
    "    \n",
    "    sentences = [tokenize(d) for d in content] #논문 하나가 한 리스트에 토큰화\n",
    "    return sentences\n",
    "\n",
    "def open_file(file): #txt 파일 오픈\n",
    "    txt_file = open(file, encoding='utf-8')\n",
    "    content = txt_file.read()\n",
    "    txt_file.close()\n",
    "    return content\n",
    "\n",
    "def preprocessing(content): #전처리\n",
    "    content = re.sub('\\\\xa0', '', content)\n",
    "    content = re.sub('\\\\n', '', content)\n",
    "    content = re.sub('\\\\\\\\xa0', '', content)\n",
    "    content = re.sub('\\\\\\\\n', '', content)\n",
    "    content = re.sub('[\\{\\}\\[\\]\\/?.,;:|\\)*~`!^\\-_+<>@#$%&\\\\\\=\\(\\'\\\")]', '', content)\n",
    "    content = ' '.join(content.split())\n",
    "                     \n",
    "    return content\n",
    "\n",
    "def tokenize(doc): #토크나이징\n",
    "    return ['/'.join(t) for t in t.pos(doc, norm=True, stem=True)]\n",
    "\n",
    "def wv_update(file): #새로운 논문 업데이트\n",
    "    name=os.path.splitext(file)[-2]+'.txt'\n",
    "    os.system(\"python pdfminer.six-20170720/tools/pdf2txt.py -o %s%s %s%s\" %(TXT_PATH, name, PDF_PATH, file))\n",
    "    \n",
    "    new_token=[]\n",
    "    content.append(preprocessing(open_file(TXT_PATH + name)))\n",
    "    \n",
    "    \n",
    "    new_token=[tokenize(d) for d in content]\n",
    "    \n",
    "    wv_model_bio.build_vocab(new_token, update=True)\n",
    "    wv_model_bio.train(new_token, total_examples=wv_model_bio.corpus_count, epochs=wv_model_bio.epochs)\n",
    "    wv_model_bio.init_sims(replace=True)\n",
    "    now=datetime.now()\n",
    "    wv_model_bio.save('%s%s%s%s%s%s' % (SAVE_PATH ,now.year, now.month, now.day, now.hour, now.minute))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "pdf_to_text(PDF_PATH) #pdf 파일 txt로 변환 실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = (open_file('./bio_txt/알도스테론_탈출에_동반된_신장의_나트륨_이뇨_입장_변화.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = preprocessing(open_file('./bio_txt/생물정보학을_이용한_인체_감염주요_플라비바이러스_공통백신_후보군_도출.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "sentences=get_file(TXT_PATH) #txt파일 read 후 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "wv_model_bio = Word2Vec(sentences, size=100, window = 8, min_count=10, workers=4, iter=100, sg=1)\n",
    "                 #100차원 벡터, 주변단어 8개 참조, 출연빈도 10번이상, CPU 쿼드코어, 100번 반복 , Skip-Gram학습\n",
    "now=datetime.now()\n",
    "wv_model_bio.save('%s%s%s%s%s%s' % (SAVE_PATH, now.year, now.month, now.day, now.hour, now.minute))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenize('한국어 자연어 처리 시스템을 구현해보겠습니다')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wv_model_bio.corpus_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "len(wv_model_bio.wv.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "wv_model_bio.wv.most_similar(tokenize('단백질'), topn=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wv_model_bio.wv.similarity(*tokenize('단백질 단백질'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenize('소설 명사')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "wv_model_bio.predict_output_word(tokenize('프로테오믹스'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import font_manager, rc\n",
    "font_fname='c:/windows/fonts/H2GTRM.TTF'\n",
    "font_name=font_manager.FontProperties(fname=font_fname).get_name()\n",
    "rc('font', family=font_name)\n",
    "\n",
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment = None \n",
    "import numpy as np\n",
    "import nltk\n",
    "\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def tsne_plot(model):\n",
    "    \"Creates and TSNE model and plots it\"\n",
    "    labels = []\n",
    "    tokens = []\n",
    "\n",
    "    for word in model.wv.vocab:\n",
    "        tokens.append(model[word])\n",
    "        labels.append(word)\n",
    "    \n",
    "    tsne_model = TSNE(perplexity=40, n_components=2, init='pca', n_iter=2500, random_state=23)\n",
    "    new_values = tsne_model.fit_transform(tokens)\n",
    "\n",
    "    x = []\n",
    "    y = []\n",
    "    for value in new_values:\n",
    "        x.append(value[0])\n",
    "        y.append(value[1])\n",
    "        \n",
    "    plt.figure(figsize=(16, 16)) \n",
    "    for i in range(len(x)):\n",
    "        plt.scatter(x[i],y[i])\n",
    "        plt.annotate(labels[i],\n",
    "                     xy=(x[i], y[i]),\n",
    "                     xytext=(5, 2),\n",
    "                     textcoords='offset points',\n",
    "                     ha='right',\n",
    "                     va='bottom')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne_plot(wv_model_bio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import font_manager, rc\n",
    "font_fname='c:/windows/fonts/H2GTRM.TTF'\n",
    "font_name=font_manager.FontProperties(fname=font_fname).get_name()\n",
    "rc('font', family=font_name)\n",
    "\n",
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment = None \n",
    "import numpy as np\n",
    "import nltk\n",
    "\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def display_closestwords_tsnescatterplot(model, word):\n",
    "    \n",
    "    arr = np.empty((0,100), dtype='f')\n",
    "    word_labels = [word]\n",
    "\n",
    "    # get close words\n",
    "    close_words = model.similar_by_word(word)\n",
    "    \n",
    "    # add the vector for each of the closest words to the array\n",
    "    arr = np.append(arr, np.array([model[word]]), axis=0)\n",
    "    for wrd_score in close_words:\n",
    "        wrd_vector = model[wrd_score[0]]\n",
    "        word_labels.append(wrd_score[0])\n",
    "        arr = np.append(arr, np.array([wrd_vector]), axis=0)\n",
    "        \n",
    "    # find tsne coords for 2 dimensions\n",
    "    tsne = TSNE(n_components=2, random_state=0)\n",
    "    np.set_printoptions(suppress=True)\n",
    "    Y = tsne.fit_transform(arr)\n",
    "\n",
    "    x_coords = Y[:, 0]\n",
    "    y_coords = Y[:, 1]\n",
    "    # display scatter plot\n",
    "    plt.scatter(x_coords, y_coords)\n",
    "\n",
    "    for label, x, y in zip(word_labels, x_coords, y_coords):\n",
    "        plt.annotate(label, xy=(x, y), xytext=(0, 0), textcoords='offset points')\n",
    "    plt.xlim(x_coords.min()+0.00005, x_coords.max()+0.00005)\n",
    "    plt.ylim(y_coords.min()+0.00005, y_coords.max()+0.00005)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time\n",
    "display_closestwords_tsnescatterplot(wv_model_bio, *tokenize('유전자'))\n",
    "#wv_model_bio.wv.most_similar(tokenize('유전자'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pname='발포폴리프로필렌_여재가_충진된_생물여과공정을_이용한_2차_처리수_질소_제거에_관한_연구'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.system(\"python pdfminer.six-20170720/tools/pdf2txt.py -o %s%s.txt %s%s.pdf\" %(TXT_PATH,pname, PDF_PATH,pname))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non=open('./bio_txt/%s.txt' % pname, encoding='utf-8').read()\n",
    "\n",
    "content2=[]\n",
    "\n",
    "content2.append(preprocessing(non)) #content리스트 하나에 하나의 논문\n",
    "\n",
    "non_ko = [tokenize(d) for d in content2]  #논문 하나가 한 리스트에 토큰화\n",
    "\n",
    "non_ko"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non2=open('./bio_txt/%s.txt' % pname, encoding='utf-8').read()\n",
    "\n",
    "content3=[]\n",
    "\n",
    "content3.append(preprocessing(non2)) #content리스트 하나에 하나의 논문\n",
    "\n",
    "non_ko2 = [tokenize(d) for d in content3]  #논문 하나가 한 리스트에 토큰화\n",
    "\n",
    "non_ko2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
