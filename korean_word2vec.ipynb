{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Twitter\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from copy import deepcopy\n",
    "import re\n",
    "import os\n",
    "from datetime import datetime\n",
    "import csv\n",
    "import sys \n",
    "\n",
    "t=Twitter()\n",
    "\n",
    "TXT_PATH='./korea_txt/'\n",
    "PDF_PATH='./korea_pdf/'\n",
    "SAVE_PATH='./korea_wv/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pdf_to_text(path):    #pdf 파일을 txt 파일로\n",
    "    pdf_list=[]\n",
    "    pdfs=os.listdir(PDF_PATH)\n",
    "    for pdf in pdfs:\n",
    "        ext = os.path.splitext(pdf)[-1]\n",
    "        name=os.path.splitext(pdf)[-2]+'.txt'\n",
    "        \n",
    "        if ext=='.pdf':\n",
    "            #pdf_list.append(\"%s%s\" % (path, pdf))\n",
    "            #pdf_name.append(\"%s\"%pdf)\n",
    "            os.system(\"python pdfminer.six-20170720/tools/pdf2txt.py -o %s%s %s%s\" %(TXT_PATH, name, PDF_PATH, pdf))\n",
    "\n",
    "def get_file(path): #txt파일 불러오기\n",
    "    filelist=[]\n",
    "    files = os.listdir(TXT_PATH) # txt파일에 있는 파일 리스트를 가져옴\n",
    "    for file in files:\n",
    "        ext = os.path.splitext(file)[-1]\n",
    "            \n",
    "        if ext == '.txt':\n",
    "            filelist.append(\"%s%s\" % (path, file))\n",
    "    \n",
    "    content=[]\n",
    "    for file in filelist:\n",
    "        content.append(preprocessing(open_file(file))) #content리스트 하나에 하나의 논문\n",
    "    \n",
    "    sentences = [tokenize(d) for d in content] #논문 하나가 한 리스트에 토큰화\n",
    "    return sentences\n",
    "\n",
    "def open_file(file): #txt 파일 오픈\n",
    "    txt_file = open(file, encoding='utf-8')\n",
    "    content = txt_file.read()\n",
    "    txt_file.close()\n",
    "    return content\n",
    "\n",
    "def preprocessing(content): #전처리\n",
    "    content = re.sub('\\\\xa0', '', content)\n",
    "    content = re.sub('\\\\n', '', content)\n",
    "    content = re.sub('\\\\\\\\xa0', '', content)\n",
    "    content = re.sub('\\\\\\\\n', '', content)\n",
    "    content = re.sub('[\\{\\}\\[\\]\\/?.,;:|\\)*~`!^\\-_+<>@#$%&\\\\\\=\\(\\'\\\")]', '', content)\n",
    "    content = ' '.join(content.split())\n",
    "                     \n",
    "    return content\n",
    "\n",
    "def tokenize(doc): #토크나이징\n",
    "    return ['/'.join(t) for t in t.pos(doc, norm=True, stem=True)]\n",
    "\n",
    "def wv_update(file): #새로운 논문 업데이트\n",
    "    name=os.path.splitext(file)[-2]+'.txt'\n",
    "    os.system(\"python pdfminer.six-20170720/tools/pdf2txt.py -o %s%s %s%s\" %(TXT_PATH, name, PDF_PATH, file))\n",
    "    \n",
    "    new_token=[]\n",
    "    content.append(preprocessing(open_file(TXT_PATH + name)))\n",
    "    \n",
    "    \n",
    "    new_token=[tokenize(d) for d in content]\n",
    "    \n",
    "    wv_model_ko.build_vocab(new_token, update=True)\n",
    "    wv_model_ko.train(new_token, total_examples=wv_model_ko.corpus_count, epochs=wv_model_ko.epochs)\n",
    "    wv_model_ko.init_sims(replace=True)\n",
    "    now=datetime.now()\n",
    "    wv_model_ko.save('%s%s%s%s%s%s' % (SAVE_PATH ,now.year, now.month, now.day, now.hour, now.minute))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#pdf_to_text(PDF_PATH) #pdf 파일 txt로 변환 실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "sentences=get_file(TXT_PATH) #txt파일 read 후 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "wv_model_ko = Word2Vec(sentences, size=100, window = 8, min_count=10, workers=4, iter=100, sg=1)\n",
    "#100차원 벡터, 주변단어 2개 참조, 출연빈도 20번이상, CPU 쿼드코어, 100번 반복, CBOW, Skip-Gram중 후자\n",
    "now=datetime.now()\n",
    "wv_model_ko.save('%s%s%s%s%s%s' % (SAVE_PATH, now.year, now.month, now.day, now.hour, now.minute))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wv_model_ko.corpus_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "len(wv_model_ko.wv.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "wv_model_ko.wv.most_similar(tokenize('문학'), topn=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "wv_model_ko.wv.similarity(*tokenize('엄마 소녀'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "wv_model_ko.predict_output_word(tokenize('프로테오믹스'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import font_manager, rc\n",
    "font_fname='c:/windows/fonts/H2GTRM.TTF'\n",
    "font_name=font_manager.FontProperties(fname=font_fname).get_name()\n",
    "rc('font', family=font_name)\n",
    "\n",
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment = None \n",
    "import numpy as np\n",
    "import nltk\n",
    "\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def tsne_plot(model):\n",
    "    \"Creates and TSNE model and plots it\"\n",
    "    labels = []\n",
    "    tokens = []\n",
    "\n",
    "    for word in model.wv.vocab:\n",
    "        tokens.append(model[word])\n",
    "        labels.append(word)\n",
    "    \n",
    "    tsne_model = TSNE(perplexity=40, n_components=2, init='pca', n_iter=2500, random_state=23)\n",
    "    new_values = tsne_model.fit_transform(tokens)\n",
    "\n",
    "    x = []\n",
    "    y = []\n",
    "    for value in new_values:\n",
    "        x.append(value[0])\n",
    "        y.append(value[1])\n",
    "        \n",
    "    plt.figure(figsize=(16, 16)) \n",
    "    for i in range(len(x)):\n",
    "        plt.scatter(x[i],y[i])\n",
    "        plt.annotate(labels[i],\n",
    "                     xy=(x[i], y[i]),\n",
    "                     xytext=(5, 2),\n",
    "                     textcoords='offset points',\n",
    "                     ha='right',\n",
    "                     va='bottom')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import font_manager, rc\n",
    "font_fname='c:/windows/fonts/H2GTRM.TTF'\n",
    "font_name=font_manager.FontProperties(fname=font_fname).get_name()\n",
    "rc('font', family=font_name)\n",
    "\n",
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment = None \n",
    "import numpy as np\n",
    "import nltk\n",
    "\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def display_closestwords_tsnescatterplot(model, word):\n",
    "    \n",
    "    arr = np.empty((0,100), dtype='f')\n",
    "    word_labels = [word]\n",
    "\n",
    "    # get close words\n",
    "    close_words = model.similar_by_word(word)\n",
    "    \n",
    "    # add the vector for each of the closest words to the array\n",
    "    arr = np.append(arr, np.array([model[word]]), axis=0)\n",
    "    for wrd_score in close_words:\n",
    "        wrd_vector = model[wrd_score[0]]\n",
    "        word_labels.append(wrd_score[0])\n",
    "        arr = np.append(arr, np.array([wrd_vector]), axis=0)\n",
    "        \n",
    "    # find tsne coords for 2 dimensions\n",
    "    tsne = TSNE(n_components=2, random_state=0)\n",
    "    np.set_printoptions(suppress=True)\n",
    "    Y = tsne.fit_transform(arr)\n",
    "\n",
    "    x_coords = Y[:, 0]\n",
    "    y_coords = Y[:, 1]\n",
    "    # display scatter plot\n",
    "    plt.scatter(x_coords, y_coords)\n",
    "\n",
    "    for label, x, y in zip(word_labels, x_coords, y_coords):\n",
    "        plt.annotate(label, xy=(x, y), xytext=(0, 0), textcoords='offset points')\n",
    "    plt.xlim(x_coords.min()+0.00005, x_coords.max()+0.00005)\n",
    "    plt.ylim(y_coords.min()+0.00005, y_coords.max()+0.00005)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time\n",
    "display_closestwords_tsnescatterplot(wv_model_ko, *tokenize('소설'))\n",
    "#wv_model_bio.wv.most_similar(tokenize('유전자'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non=open('./txt/생식생물학에서_프로테오믹스의_응용.txt', encoding='utf-8').read()\n",
    "\n",
    "content2=[]\n",
    "\n",
    "content2.append(preprocessing(non)) #content리스트 하나에 하나의 논문\n",
    "\n",
    "non_ko = [tokenize(d) for d in content2]  #논문 하나가 한 리스트에 토큰화\n",
    "\n",
    "non_ko"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
